"""
/**
 * FILE: run_pipeline.py
 * VAI TRÃ’: Äiá»ƒm Äáº§u VÃ o (Entry Point) Duy Nháº¥t dÃ nh cho cáº£ Backend CLI vÃ  Giao diá»‡n UI (Streamlit).
 * CHá»¨C NÄ‚NG:
 * - Äiá»u phá»‘i quy trÃ¬nh náº¡p dá»¯ liá»‡u (ETL).
 * - Huáº¥n luyá»‡n cÃ¡c Agent há»c mÃ¡y (Training).
 * - Kiá»ƒm thá»­ lá»‹ch sá»­ giao dá»‹ch (Backtesting).
 * - Khá»Ÿi cháº¡y Giao diá»‡n Trá»±c quan (UI).
 */
"""
import os
import sys

# MONKEY PATCH TENSORBOARD NATIVE KERNEL: Báºº GÃƒY LUáº¬T LÆ¯U FILE CÃ“ HOSTNAME/TIMESTAMP
try:
    import tensorboard.summary.writer.event_file_writer as tb_efw
    import time
    
    class CleanEventFileWriter(tb_efw.EventFileWriter):
        def __init__(self, logdir, max_queue_size=10, flush_secs=120, filename_suffix=""):
            self._logdir = logdir
            
            # TÃªn tÄ©nh tuyá»‡t Ä‘á»‘i. Náº¿u gá»i nhiá»u láº§n trong 1 giÃ¢y, ta thÃªm timestamp siÃªu ngáº¯n thay vÃ¬ Desktop ID dÃ i thÆ°á»£t.
            # Hoáº·c Ã©p thÃ nh "events.out.tfevents.alphaquant"
            self._file_name = os.path.join(logdir, "events.out.tfevents.alphaquant")
            
            # Náº¿u chÃ©p Ä‘Ã¨ thÃ¬ xÃ³a rÃ¡c cÅ©
            if os.path.exists(self._file_name):
                try: os.remove(self._file_name)
                except: pass
                
            # The RecordWriter expects a file-like object
            self._file_writer = open(self._file_name, "wb")
            self._ev_writer = tb_efw.RecordWriter(self._file_writer)
            self._async_writer = tb_efw._AsyncWriter(self._ev_writer, max_queue_size, flush_secs)
            self._closed = False
            
    # Ã‰p ghi Ä‘Ã¨ toÃ n há»‡ thá»‘ng Python hiá»‡n táº¡i
    tb_efw.EventFileWriter = CleanEventFileWriter
except ImportError:
    pass

os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Suppress TensorFlow oneDNN warning
# Báº¯t buá»™c Ã©p thÆ° má»¥c lÃ m viá»‡c hiá»‡n táº¡i (CWD) vá» Rá»… cá»§a Dá»± Ã¡n Ä‘á»ƒ trÃ¡nh lá»—i Ä‘Æ°á»ng dáº«n trÃªn Windows
os.chdir(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import argparse

def get_assets(dir_path):
    if not os.path.exists(dir_path): return []
    return [f.split('.')[0] for f in os.listdir(dir_path) if f.endswith('.csv')]

def main():
    from src.utils.logger import logger
    from src.utils.config_loader import config
    
    parser = argparse.ArgumentParser(description="AlphaQuantAgent: Master Orchestrator (The Singularity)")
    parser.add_argument("--epochs", type=int, default=1000,
                        help="Sá»‘ vÃ²ng láº·p huáº¥n luyá»‡n tá»‘i Ä‘a (Ã©p Early Stopping náº¿u cáº§n)")
    parser.add_argument("--sims", type=int, default=1,
                        help="Sá»‘ lÆ°á»£ng persona cho Backtest (máº·c Ä‘á»‹nh 1)")
    parser.add_argument("--force", action="store_true",
                        help="Ã‰p buá»™c train láº¡i tá»« Ä‘áº§u bá» qua cache mÃ´ hÃ¬nh")
    parser.add_argument("--test-mode", action="store_true",
                        help="KÃ­ch hoáº¡t cháº¿ Ä‘á»™ test pipeline siÃªu tá»‘c")
                        
    args, unknown = parser.parse_known_args()
    
    logger.info("=========================================================================")
    logger.info("                  ALPHAQUANT MASTER PIPELINE INITIATED                   ")
    logger.info("=========================================================================")

    # BÆ¯á»šC 1: KHá»I Táº O PERSONA (PERSONA GENERATION)
    logger.info("\n>>> [STEP 1/5] KHá»I Táº O VÃ€ CHUáº¨N Bá»Š PERSONA...")
    
    model_path = "models/rl_agent/best_model.zip"
    if args.force and os.path.exists(model_path):
        try: os.remove(model_path)
        except: pass

    config_sys = config.system
    data_dir = config_sys.get('data_paths', {}).get('base_dir', 'data/')
    
    trade_assets = get_assets(os.path.join(data_dir, 'trades'))
    rate_assets = get_assets(os.path.join(data_dir, 'rates'))
    stat_assets = get_assets(os.path.join(data_dir, 'stats'))
    context_assets = rate_assets + stat_assets

    import numpy as np
    from src.engine.persona_generator import PersonaGenerator
    from src.pipeline.batch_loader import SmartBatchLoader
    
    is_test_mode = getattr(args, 'test_mode', False)
    if is_test_mode:
        total_personas_to_gen = 2
        batch_size = 5
        epochs_total = 5000
        n_simulations = 1
    else:
        total_personas_to_gen = 50
        batch_size = 20
        epochs_total = args.epochs
        n_simulations = args.sims

    generator = PersonaGenerator(trade_assets, rate_assets, stat_assets)
    dataset = generator.generate_dataset(n_personas=total_personas_to_gen)
    np.random.shuffle(dataset)

    # Chia táº­p Train vÃ  táº­p Backtest (Validation)
    train_personas = dataset[:-n_simulations] if len(dataset) > n_simulations else dataset
    eval_persona = dataset[0] # Äáº¡i diá»‡n 1 ngÆ°á»i cho EvalCallback
    backtest_personas = dataset[-n_simulations:] if len(dataset) >= n_simulations else dataset

    # BÆ¯á»šC 2: HUáº¤N LUYá»†N (TRAINING)
    logger.info(f"\n>>> [STEP 2/5] Báº®T Äáº¦U HUáº¤N LUYá»†N PPO TRÃŠN {len(train_personas)} PERSONAS...")
    
    from src.engine.market import Market
    from src.engine.simulator import TradingSimulator
    from src.engine.wallet import Wallet
    from src.engine.env import AlphaQuantEnv
    from src.agents.trader import RLTrader
    from src.agents.callbacks import EarlyStoppingAndLogging
    
    from stable_baselines3.common.vec_env import SubprocVecEnv
    from stable_baselines3.common.callbacks import EvalCallback
    from stable_baselines3.common.callbacks import CallbackList
    import gc

    def make_env(persona, mega_matrix, master_timeline):
        def _init():
            import shutup
            shutup.please() 
            p_market = Market(
                asset_list=persona['trade_assets'] + rate_assets, 
                context_list=context_assets, 
                data_path=data_dir,
                pre_aligned_matrix=mega_matrix,
                pre_aligned_timeline=master_timeline
            )
            p_market.load()
            p_market.inject_ml_features()
            proxy_penalty_beta = persona['drawdown_penalty'] / 100.0  
            wallet = Wallet(initial_capital=persona['initial_capital'], penalty_beta=proxy_penalty_beta)
            sim = TradingSimulator(p_market, wallet, window_size=config.models.get('window_size', 16), allowed_trade_assets=persona['trade_assets'], persona=persona)
            env = AlphaQuantEnv(sim)
            return env
        return _init

    has_agent_instantiated = False
    
    if not os.path.exists(model_path) or args.force:
        loader = SmartBatchLoader(data_dir, batch_size=batch_size)
        for batch_id, batch_symbols, mega_matrix, master_timeline in loader.get_batches():
            if mega_matrix.empty: continue
            
            active_personas = [p for p in train_personas if set(p['trade_assets']).intersection(set(batch_symbols))]
            if not active_personas: continue
            active_personas = active_personas[:8]  # Giá»›i háº¡n sá»‘ luá»“ng (an toÃ n cho RAM)
            
            logger.info(f"    -> Äang Train Batch {batch_id} vá»›i {len(active_personas)} Vector MÃ´i trÆ°á»ng Ä‘a chiá»u...")
            
            env_fns = [make_env(p, mega_matrix, master_timeline) for p in active_personas]
            vec_env = SubprocVecEnv(env_fns)
            
            eval_env_fn = make_env(eval_persona, mega_matrix, master_timeline)
            eval_env = SubprocVecEnv([eval_env_fn])
            
            eval_callback = EvalCallback(eval_env, best_model_save_path='models/rl_agent/',
                                         log_path='logs/training/tensorboard/ppo_eval/', eval_freq=100 if is_test_mode else 1000,
                                         deterministic=True, render=False)
            omni_cb = EarlyStoppingAndLogging(check_freq=50, log_dir="logs/training/tensorboard/ppo/")
            callbacks = CallbackList([eval_callback, omni_cb])
            
            if not has_agent_instantiated:
                agent = RLTrader(vec_env)
                has_agent_instantiated = True
            else:
                agent.model.set_env(vec_env)
                
            try:
                agent.learn(total_timesteps=epochs_total, callbacks=callbacks)
                agent.save(model_path)
            finally:
                vec_env.close()
                eval_env.close()
                del vec_env, eval_env, mega_matrix, master_timeline
                Market._raw_cache.clear()
                gc.collect()
            
            if is_test_mode: break
    else:
        logger.info("    -> ÄÃ£ tÃ¬m tháº¥y model Ä‘Æ°á»£c huáº¥n luyá»‡n sáºµn trong bá»™ nhá»›. Bá» qua Training PPO Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian.")

    # BÆ¯á»šC 3: KIá»‚M THá»¬ GIAO Dá»ŠCH (BACKTESTING)
    logger.info(f"\n>>> [STEP 3/5] CHáº Y KIá»‚M THá»¬ TRÃŠN Táº¬P VALIDATION ({len(backtest_personas)} PERSONAS)...")
    
    global_trade_history = []
    global_assets_traded = set()
    global_feature_keys = []
    last_portfolio_history = []
    
    for sim_idx, p in enumerate(backtest_personas):
        logger.info(f"    -> Äang mÃ´ phá»ng giao dá»‹ch cho Validation Persona {sim_idx+1}/{len(backtest_personas)}...")
        global_assets_traded.update(p['trade_assets'])
        
        p_market = Market(asset_list=p['trade_assets'] + rate_assets, context_list=context_assets, data_path=data_dir)
        p_market.load()
        p_market.inject_ml_features()
        
        if not global_feature_keys and hasattr(p_market, 'feature_map'):
            global_feature_keys = list(p_market.feature_map.keys())
        
        wallet = Wallet(initial_capital=p['initial_capital'], penalty_beta=p['drawdown_penalty']/100.0)
        sim = TradingSimulator(p_market, wallet, window_size=config.models.get('window_size', 16), allowed_trade_assets=p['trade_assets'])
        env = AlphaQuantEnv(sim)

        agent = RLTrader(env, model_path=model_path if os.path.exists(model_path) else None)
        obs, _ = env.reset()
        done = False
        nav_history = []
        
        while not done:
            action = agent.predict(obs)
            obs, reward, done, truncated, info = env.step(action)
            nav_history.append(info['nav'])
            
        if sim_idx == len(backtest_personas) - 1:
            wallet.export_csv("logs/trading/transactions.csv")
            
        global_trade_history.extend(wallet.ledger)
        last_portfolio_history = nav_history
        
        env.close()
        del agent, env, sim, wallet, p_market
        gc.collect()

    import pandas as pd
    from src.engine.quant_analyzer import calculate_advanced_metrics
    
    df_market = pd.DataFrame()
    for asset in global_assets_traded:
        if asset in Market._raw_cache:
            df_asset = Market._raw_cache[asset]
            if 'close' in df_asset.columns:
                if df_market.empty:
                    df_market[asset] = df_asset['close']
                else:
                    df_market = df_market.join(df_asset['close'].rename(asset), how='outer')
    df_market = df_market.ffill().fillna(0)

    # BÆ¯á»šC 4: MONTE CARLO STRESS TEST
    logger.info("\n>>> [STEP 4/5] CHáº Y MÃ” PHá»NG LÆ¯á»¢NG Tá»¬ MONTE CARLO TRÃŠN Lá»ŠCH Sá»¬ THá»°C Táº¾...")
    from src.engine.monte_carlo import MonteCarloSimulator
    mc_report = None
    
    if len(last_portfolio_history) > 2:
        returns = pd.Series(last_portfolio_history).pct_change().dropna().values
        mc = MonteCarloSimulator(n_paths=1000, horizon_steps=252*4) 
        mu, sigma = mc.estimate_drift_and_vol(returns)
        S0 = last_portfolio_history[-1]
        try:
            mc_report = mc.run_stress_test(agent=None, initial_capital=S0, S0=S0, mu=mu, sigma=sigma)
            logger.info("    -> Monte Carlo hoÃ n táº¥t. TÃ­nh toÃ¡n rá»§i ro thÃ nh cÃ´ng.")
        except Exception as e:
            logger.error(f"    -> Lá»—i khi cháº¡y Monte Carlo: {e}")
    else:
        logger.warning("    -> KhÃ´ng Ä‘á»§ dá»¯ liá»‡u sinh lá»i Ä‘á»ƒ cháº¡y Monte Carlo.")

    # BÆ¯á»šC 5: TÃ“M Gá»ŒN CHá»ˆ Sá» JSON (THE GOLDEN DUMP)
    logger.info("\n>>> [STEP 5/5] Tá»”NG Há»¢P SIÃŠU CHá»ˆ Sá» Äá»ŠNH LÆ¯á»¢NG (THE GOLDEN DUMP)...")
    calculate_advanced_metrics(last_portfolio_history, global_trade_history, df_market, feature_map_keys=global_feature_keys, mc_report=mc_report)
    
    Market._raw_cache.clear()
    gc.collect()
            
    logger.info("\n=========================================================================")
    logger.info("ğŸ‰ ÄÆ¯á»œNG á»NG Dá»® LIá»†U HOÃ€N Táº¤T. [advanced_quant_metrics.json] ÄÃƒ Sáº´N SÃ€NG CHO REACT/NODE!")
    logger.info("=========================================================================\n")

if __name__ == "__main__":
    main()
