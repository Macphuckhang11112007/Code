"""
/**
 * MODULE: System UI - Training Monitor
 * VAI TR√í: Nghe l√©n (Eavesdrop) v√†o th∆∞ m·ª•c logs/training/tensorboard/ ƒë·ªÉ l·∫•y s·ªë li·ªáu Reward tr·ª±c ti·∫øp.
 * T·∫†I SAO: Lo·∫°i b·ªè s·ª± ph·ª• thu·ªôc ph·∫£i m·ªü Server Tensorboard c·ªïng 6006 r∆∞·ªùm r√†. Dev c√≥ th·ªÉ xem AI h·ªçc t·ªõi ƒë√¢u ngay tr√™n Web App.
 */
"""
import streamlit as st
import pandas as pd
import plotly.express as px
import os
import glob

def render_tensorboard_stats():
    st.header("ü§ñ AI Convergence Monitor (Learning Curve)")
    st.info("Live Tracking of Reinforcement Learning Episodes. Visualizing Reward Optimization through Proximal Policy Optimization (PPO).")
    
    # T√¨m ki·∫øm file metric CSV m·ªõi nh·∫•t trong logs/training
    log_dir = "logs/training/tensorboard"
    if not os.path.exists(log_dir):
        st.warning(f"Ch∆∞a c√≥ D·ªØ li·ªáu Hu·∫•n Luy·ªán t·∫°i `{log_dir}`. H√£y ch·∫°y l·ªánh `python main.py --mode train` tr∆∞·ªõc.")
        return
        
    # Ki·ªÉm tra tr·ª±c ti·∫øp ƒë∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh
    default_csv = os.path.join(log_dir, "training_metrics.csv")
    if os.path.exists(default_csv):
        latest_file = default_csv
    else:
        # Fallback t√¨m ki·∫øm
        list_of_files = glob.glob(f"{log_dir}/**/training_metrics.csv", recursive=True)
        if not list_of_files:
            st.warning("Kh√¥ng t√¨m th·∫•y file `training_metrics.csv` n√†o trong h·ªá th·ªëng logs.")
            return
        latest_file = max(list_of_files, key=os.path.getctime)
        
    st.caption(f"ƒêang hi·ªÉn th·ªã Log t·∫°i: `{latest_file}`")
    
    df = pd.read_csv(latest_file)
    if 'step' not in df.columns or 'reward' not in df.columns:
        st.error("File CSV sai ƒë·ªãnh d·∫°ng. C·∫ßn c·ªôt `step` v√† `reward`.")
        return
        
    # T√≠nh ƒë∆∞·ªùng EMA ƒë·ªÉ cho ƒë∆∞·ªùng cong m∆∞·ª£t m√† (Smoothed Learning Curve)
    df['reward_smoothed'] = df['reward'].ewm(span=50, adjust=False).mean()
    
    current_reward = df['reward'].iloc[-1]
    prev_reward = df['reward'].iloc[-2] if len(df) > 1 else current_reward
    delta_reward = current_reward - prev_reward
    
    metric_color = "normal" if delta_reward >= 0 else "inverse"
    st.metric(label="Reward Epoch Ho√†n T·∫•t", value=f"{current_reward:.4f}", delta=f"{delta_reward:.4f}", delta_color=metric_color)
    
    # V·∫Ω Bi·ªÉu ƒë·ªì B·∫±ng Plotly
    fig = px.line(df, x='step', y=['reward', 'reward_smoothed'], 
                  labels={'value': 'PPO Reward', 'step': 'Timesteps / Epochs'},
                  color_discrete_map={'reward': '#30363d', 'reward_smoothed': '#00ff9d'})
                  
    fig.update_layout(
        title="Qu√° Tr√¨nh Ti·∫øn H√≥a (Deep RL PPO Convergence)",
        template="plotly_dark",
        plot_bgcolor="#0e1117",
        paper_bgcolor="#0e1117",
        height=400,
        legend_title_text='Trace'
    )
    
    st.plotly_chart(fig, width="stretch", key="learning_curve_chart")
